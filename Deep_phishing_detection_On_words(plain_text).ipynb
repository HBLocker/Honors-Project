{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_Deep_phishing_detection_On_words(plain_text).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y_GWA62m4go",
        "colab_type": "code",
        "outputId": "6c96521e-2c3d-4aa8-ba70-d012e354a8b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "!pip3 install keras sklearn tqdm numpy keras_metrics tensorflow==1.14\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n",
            "Collecting keras_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 28kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 78.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.28.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.2.1)\n",
            "Installing collected packages: keras-metrics, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0rc3\n",
            "    Uninstalling tensorflow-2.2.0rc3:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Successfully installed keras-metrics-1.1.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRmOE0v46MFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAFmN82Rtx_8",
        "colab_type": "code",
        "outputId": "7c3ccdc6-706b-4aba-da0d-4b57e8e949f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z96EKXY8Ey4r",
        "colab_type": "code",
        "outputId": "f7d368da-1fbe-49af-9bde-ced696bf7c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.callbacks import History \n",
        "history = History()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMMHAZbSnQoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install keras\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense, Dropout, Activation,Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW01W4ZQnUZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend\n",
        "from keras.layers import Dense\n",
        "from keras import Sequential\n",
        "from keras.utils import  plot_model\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import time\n",
        "import sklearn.metrics as metrics\n",
        "from matplotlib.pyplot import *\n",
        "from matplotlib import pyplot as plt\n",
        "#import keras_metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9UIobOpqTKM",
        "colab_type": "code",
        "outputId": "d18224a4-4a60-4dc2-fe53-9afac0faa097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install keras-metrics\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-metrics in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras-metrics) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.18.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Ujj0-T7_AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras_metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccBHz5UEnX_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import model_from_json\n",
        "from matplotlib.pyplot import *\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sykJ1kwnnsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LENGTH = 100\n",
        "EMBEDDING_SIZE = 300\n",
        "TEST_SIZE = 0.5\n",
        "FILTERS = 70\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "EPOCHS =1  # number of epochs\n",
        "# give lables a numeric value\n",
        "label2int = {\"ham\": 0, \"spam\": 1}\n",
        "int2label = {0: \"ham\", 1: \"spam\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYDvL0C4c1J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 5000\n",
        "maxlen = 100\n",
        "batch_size = 100\n",
        "embedding_dims = 50\n",
        "filters = 250\n",
        "kernel_size = 25\n",
        "hidden_dims = 250\n",
        "epochs = 300\n",
        "pool_size = 15\n",
        "lstm_output_size = 1028\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3mLB_ucnqo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Load_data(): #loads the data in as UTF8-sig dk what sig is but stops it breaking \n",
        "  text, labels = [], []\n",
        "  with open(\"datatest\",encoding='utf-8-sig') as f:\n",
        "    for line in f:\n",
        "      split = line.split()\n",
        "      labels.append(split[0].strip())\n",
        "      text.append(''.join(split[1:]).strip())\n",
        "  return text,labels "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCb_-LFrnzr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = Load_data() # loads the x and Y data \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuzLOZKen2g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer() #converts the utf-8 into tokinized characters \n",
        "tokenizer.fit_on_texts(X)\n",
        "# tokinize into ints \n",
        "X = tokenizer.texts_to_sequences(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koH87Wf1n4q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArZtTpdcn_MU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = [ label2int[label] for label in y ] #loads lables \n",
        "y = to_categorical(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpmMJMTtoDM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Q8USiCoFDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=43) #plit data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UviGedEdoSmi",
        "colab_type": "text"
      },
      "source": [
        "Loads glve embedding file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud3KELDHoUSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_vectors(tokenizer, dim=EMBEDDING_SIZE):\n",
        "    embedding_index = {}\n",
        "    with open(\"glove.6B.300d.txt\", 'r',encoding='utf8',errors = 'ignore') as f:\n",
        "        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vectors = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = vectors\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found will be 0s\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1l9F7VsrfW7",
        "colab_type": "text"
      },
      "source": [
        "LSTM MODEL IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdLd5_mnobLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import Input, ConvLSTM2D, BatchNormalization, RepeatVector, Conv2D\n",
        "from keras.models import Sequential, load_model\n",
        "def get_model(tokenizer, lstm_units): # builds the lstm model\n",
        "      \n",
        "    embedding_matrix = get_embedding_vectors(tokenizer) # loads glove embedding \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(tokenizer.word_index)+1, \n",
        "              EMBEDDING_SIZE,\n",
        "              weights=[embedding_matrix],\n",
        "              trainable=False,\n",
        "              input_length=SEQUENCE_LENGTH))\n",
        "   \n",
        "    model.add(LSTM(lstm_units, recurrent_dropout=0.3))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(2, activation=\"softmax\")) #probobility studff \n",
        "    \n",
        "\n",
        "    # rmsprop better than adam \n",
        "    #weights[0] = weights[0].reshape(list(reversed(weights[0].shape)))\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
        "    \n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-0sVHWLofSy",
        "colab_type": "code",
        "outputId": "61a887f3-6387-4128-8a00-eb3ec4879a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "model = get_model(tokenizer=tokenizer, lstm_units=2048)  # adds LSTM unnits to the model \n",
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading GloVe: 16569it [00:01, 12049.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "tracking <tf.Variable 'Variable:0' shape=() dtype=int32> tp\n",
            "tracking <tf.Variable 'Variable_1:0' shape=() dtype=int32> fp\n",
            "tracking <tf.Variable 'Variable_2:0' shape=() dtype=int32> tp\n",
            "tracking <tf.Variable 'Variable_3:0' shape=() dtype=int32> fn\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 300)          5302500   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 2048)              19243008  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 24,549,606\n",
            "Trainable params: 19,247,106\n",
            "Non-trainable params: 5,302,500\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye6rPz1yqVs5",
        "colab_type": "code",
        "outputId": "36d671cd-d114-41b8-c5bb-a13b46eb7892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initialize our ModelCheckpoint and TensorBoard callbacks\n",
        "# model checkpoint for saving best weights poor attempt at better learnrin model \n",
        "model_checkpoint = ModelCheckpoint(\"phish{val_loss:.2f}\", save_best_only=True,\n",
        "                                    verbose=1)\n",
        "# for better visualization\n",
        "tensorboard = TensorBoard(f\"phish{time.time()}\")\n",
        "# print our data shapes\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)\n",
        "# train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "          callbacks=[tensorboard, model_checkpoint],\n",
        "          verbose=1)\n",
        "model.fit\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape: (11911, 100)\n",
            "X_test.shape: (11912, 100)\n",
            "y_train.shape: (11911, 2)\n",
            "y_test.shape: (11912, 2)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 11911 samples, validate on 11912 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/100\n",
            "11911/11911 [==============================] - 54s 5ms/step - loss: 0.6376 - accuracy: 0.7094 - precision: 0.7009 - recall: 0.9345 - val_loss: 0.5850 - val_accuracy: 0.7015 - val_precision: 0.6956 - val_recall: 0.9753\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.58503, saving model to phish0.59\n",
            "Epoch 2/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.5447 - accuracy: 0.7330 - precision: 0.7244 - recall: 0.9766 - val_loss: 0.5686 - val_accuracy: 0.7084 - val_precision: 0.6993 - val_recall: 0.9853\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.58503 to 0.56856, saving model to phish0.57\n",
            "Epoch 3/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.5405 - accuracy: 0.7300 - precision: 0.7265 - recall: 0.9718 - val_loss: 0.5509 - val_accuracy: 0.7189 - val_precision: 0.7074 - val_recall: 0.9817\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.56856 to 0.55086, saving model to phish0.55\n",
            "Epoch 4/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.5454 - accuracy: 0.7245 - precision: 0.7163 - recall: 0.9318 - val_loss: 0.7431 - val_accuracy: 0.6737 - val_precision: 0.6708 - val_recall: 0.9976\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.55086\n",
            "Epoch 5/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.5350 - accuracy: 0.7347 - precision: 0.7149 - recall: 0.9372 - val_loss: 0.5348 - val_accuracy: 0.7284 - val_precision: 0.7138 - val_recall: 0.9820\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.55086 to 0.53477, saving model to phish0.53\n",
            "Epoch 6/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.5032 - accuracy: 0.7506 - precision: 0.7279 - recall: 0.9852 - val_loss: 0.5330 - val_accuracy: 0.7312 - val_precision: 0.7156 - val_recall: 0.9847\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.53477 to 0.53302, saving model to phish0.53\n",
            "Epoch 7/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4958 - accuracy: 0.7558 - precision: 0.7418 - recall: 0.9894 - val_loss: 0.5233 - val_accuracy: 0.7351 - val_precision: 0.7187 - val_recall: 0.9854\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.53302 to 0.52326, saving model to phish0.52\n",
            "Epoch 8/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4869 - accuracy: 0.7570 - precision: 0.7304 - recall: 0.9894 - val_loss: 0.5227 - val_accuracy: 0.7377 - val_precision: 0.7203 - val_recall: 0.9860\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.52326 to 0.52269, saving model to phish0.52\n",
            "Epoch 9/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4815 - accuracy: 0.7590 - precision: 0.7401 - recall: 0.9904 - val_loss: 0.5276 - val_accuracy: 0.7377 - val_precision: 0.7226 - val_recall: 0.9794\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.52269\n",
            "Epoch 10/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4833 - accuracy: 0.7584 - precision: 0.7384 - recall: 0.9835 - val_loss: 0.5255 - val_accuracy: 0.7391 - val_precision: 0.7218 - val_recall: 0.9888\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.52269\n",
            "Epoch 11/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4829 - accuracy: 0.7599 - precision: 0.7344 - recall: 0.9891 - val_loss: 0.5241 - val_accuracy: 0.7407 - val_precision: 0.7230 - val_recall: 0.9877\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.52269\n",
            "Epoch 12/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4771 - accuracy: 0.7622 - precision: 0.7376 - recall: 0.9933 - val_loss: 0.5193 - val_accuracy: 0.7408 - val_precision: 0.7229 - val_recall: 0.9870\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.52269 to 0.51934, saving model to phish0.52\n",
            "Epoch 13/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4761 - accuracy: 0.7616 - precision: 0.7367 - recall: 0.9912 - val_loss: 0.5191 - val_accuracy: 0.7404 - val_precision: 0.7230 - val_recall: 0.9886\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.51934 to 0.51910, saving model to phish0.52\n",
            "Epoch 14/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4736 - accuracy: 0.7616 - precision: 0.7416 - recall: 0.9934 - val_loss: 0.5211 - val_accuracy: 0.7403 - val_precision: 0.7226 - val_recall: 0.9879\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.51910\n",
            "Epoch 15/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4734 - accuracy: 0.7632 - precision: 0.7348 - recall: 0.9933 - val_loss: 0.5295 - val_accuracy: 0.7397 - val_precision: 0.7226 - val_recall: 0.9874\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.51910\n",
            "Epoch 16/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4788 - accuracy: 0.7620 - precision: 0.7432 - recall: 0.9939 - val_loss: 0.5149 - val_accuracy: 0.7392 - val_precision: 0.7230 - val_recall: 0.9827\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.51910 to 0.51488, saving model to phish0.51\n",
            "Epoch 17/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4743 - accuracy: 0.7616 - precision: 0.7423 - recall: 0.9932 - val_loss: 0.5170 - val_accuracy: 0.7408 - val_precision: 0.7233 - val_recall: 0.9861\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.51488\n",
            "Epoch 18/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4718 - accuracy: 0.7637 - precision: 0.7398 - recall: 0.9940 - val_loss: 0.5197 - val_accuracy: 0.7424 - val_precision: 0.7245 - val_recall: 0.9852\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.51488\n",
            "Epoch 19/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4713 - accuracy: 0.7627 - precision: 0.7345 - recall: 0.9904 - val_loss: 0.5257 - val_accuracy: 0.7417 - val_precision: 0.7238 - val_recall: 0.9870\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.51488\n",
            "Epoch 20/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4694 - accuracy: 0.7643 - precision: 0.7425 - recall: 0.9940 - val_loss: 0.5195 - val_accuracy: 0.7416 - val_precision: 0.7242 - val_recall: 0.9837\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.51488\n",
            "Epoch 21/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4703 - accuracy: 0.7628 - precision: 0.7373 - recall: 0.9924 - val_loss: 0.5177 - val_accuracy: 0.7414 - val_precision: 0.7226 - val_recall: 0.9866\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.51488\n",
            "Epoch 22/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4692 - accuracy: 0.7637 - precision: 0.7377 - recall: 0.9925 - val_loss: 0.5191 - val_accuracy: 0.7432 - val_precision: 0.7254 - val_recall: 0.9850\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.51488\n",
            "Epoch 23/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4687 - accuracy: 0.7637 - precision: 0.7357 - recall: 0.9936 - val_loss: 0.5167 - val_accuracy: 0.7419 - val_precision: 0.7249 - val_recall: 0.9819\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.51488\n",
            "Epoch 24/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4697 - accuracy: 0.7636 - precision: 0.7436 - recall: 0.9918 - val_loss: 0.5248 - val_accuracy: 0.7423 - val_precision: 0.7247 - val_recall: 0.9859\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.51488\n",
            "Epoch 25/100\n",
            "11911/11911 [==============================] - 53s 4ms/step - loss: 0.4721 - accuracy: 0.7625 - precision: 0.7422 - recall: 0.9911 - val_loss: 0.5145 - val_accuracy: 0.7424 - val_precision: 0.7238 - val_recall: 0.9876\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.51488 to 0.51449, saving model to phish0.51\n",
            "Epoch 26/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4714 - accuracy: 0.7626 - precision: 0.7438 - recall: 0.9925 - val_loss: 0.5186 - val_accuracy: 0.7415 - val_precision: 0.7239 - val_recall: 0.9877\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.51449\n",
            "Epoch 27/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4693 - accuracy: 0.7639 - precision: 0.7393 - recall: 0.9958 - val_loss: 0.5181 - val_accuracy: 0.7402 - val_precision: 0.7234 - val_recall: 0.9836\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.51449\n",
            "Epoch 28/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4702 - accuracy: 0.7635 - precision: 0.7379 - recall: 0.9940 - val_loss: 0.5145 - val_accuracy: 0.7412 - val_precision: 0.7229 - val_recall: 0.9881\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.51449 to 0.51447, saving model to phish0.51\n",
            "Epoch 29/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4676 - accuracy: 0.7643 - precision: 0.7409 - recall: 0.9947 - val_loss: 0.5212 - val_accuracy: 0.7402 - val_precision: 0.7234 - val_recall: 0.9828\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.51447\n",
            "Epoch 30/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4692 - accuracy: 0.7651 - precision: 0.7396 - recall: 0.9943 - val_loss: 0.5222 - val_accuracy: 0.7417 - val_precision: 0.7239 - val_recall: 0.9857\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.51447\n",
            "Epoch 31/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4691 - accuracy: 0.7639 - precision: 0.7352 - recall: 0.9937 - val_loss: 0.5201 - val_accuracy: 0.7414 - val_precision: 0.7240 - val_recall: 0.9842\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.51447\n",
            "Epoch 32/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4682 - accuracy: 0.7641 - precision: 0.7360 - recall: 0.9935 - val_loss: 0.5164 - val_accuracy: 0.7419 - val_precision: 0.7238 - val_recall: 0.9855\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.51447\n",
            "Epoch 33/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4688 - accuracy: 0.7639 - precision: 0.7447 - recall: 0.9932 - val_loss: 0.5205 - val_accuracy: 0.7410 - val_precision: 0.7230 - val_recall: 0.9879\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.51447\n",
            "Epoch 34/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4674 - accuracy: 0.7639 - precision: 0.7389 - recall: 0.9937 - val_loss: 0.5208 - val_accuracy: 0.7431 - val_precision: 0.7253 - val_recall: 0.9845\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.51447\n",
            "Epoch 35/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4672 - accuracy: 0.7647 - precision: 0.7332 - recall: 0.9945 - val_loss: 0.5209 - val_accuracy: 0.7424 - val_precision: 0.7244 - val_recall: 0.9877\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.51447\n",
            "Epoch 36/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4668 - accuracy: 0.7643 - precision: 0.7414 - recall: 0.9962 - val_loss: 0.5197 - val_accuracy: 0.7427 - val_precision: 0.7239 - val_recall: 0.9892\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.51447\n",
            "Epoch 37/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4665 - accuracy: 0.7639 - precision: 0.7368 - recall: 0.9919 - val_loss: 0.5245 - val_accuracy: 0.7432 - val_precision: 0.7255 - val_recall: 0.9853\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.51447\n",
            "Epoch 38/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4664 - accuracy: 0.7646 - precision: 0.7354 - recall: 0.9929 - val_loss: 0.5271 - val_accuracy: 0.7412 - val_precision: 0.7224 - val_recall: 0.9875\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.51447\n",
            "Epoch 39/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4700 - accuracy: 0.7632 - precision: 0.7371 - recall: 0.9925 - val_loss: 0.5178 - val_accuracy: 0.7424 - val_precision: 0.7248 - val_recall: 0.9839\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.51447\n",
            "Epoch 40/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4726 - accuracy: 0.7632 - precision: 0.7364 - recall: 0.9927 - val_loss: 0.5213 - val_accuracy: 0.7419 - val_precision: 0.7247 - val_recall: 0.9818\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.51447\n",
            "Epoch 41/100\n",
            "11911/11911 [==============================] - 53s 4ms/step - loss: 0.4676 - accuracy: 0.7637 - precision: 0.7428 - recall: 0.9943 - val_loss: 0.5174 - val_accuracy: 0.7428 - val_precision: 0.7241 - val_recall: 0.9870\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.51447\n",
            "Epoch 42/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4794 - accuracy: 0.7604 - precision: 0.7467 - recall: 0.9935 - val_loss: 0.5349 - val_accuracy: 0.7351 - val_precision: 0.7203 - val_recall: 0.9809\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.51447\n",
            "Epoch 43/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4872 - accuracy: 0.7582 - precision: 0.7306 - recall: 0.9891 - val_loss: 0.5234 - val_accuracy: 0.7398 - val_precision: 0.7242 - val_recall: 0.9813\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.51447\n",
            "Epoch 44/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4754 - accuracy: 0.7622 - precision: 0.7399 - recall: 0.9921 - val_loss: 0.5128 - val_accuracy: 0.7423 - val_precision: 0.7243 - val_recall: 0.9863\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.51447 to 0.51285, saving model to phish0.51\n",
            "Epoch 45/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4755 - accuracy: 0.7616 - precision: 0.7356 - recall: 0.9933 - val_loss: 0.5324 - val_accuracy: 0.7414 - val_precision: 0.7235 - val_recall: 0.9877\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.51285\n",
            "Epoch 46/100\n",
            "11911/11911 [==============================] - 52s 4ms/step - loss: 0.4741 - accuracy: 0.7632 - precision: 0.7497 - recall: 0.9935 - val_loss: 0.5178 - val_accuracy: 0.7410 - val_precision: 0.7237 - val_recall: 0.9814\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.51285\n",
            "Epoch 47/100\n",
            "11911/11911 [==============================] - 53s 4ms/step - loss: 0.4694 - accuracy: 0.7637 - precision: 0.7419 - recall: 0.9942 - val_loss: 0.5192 - val_accuracy: 0.7424 - val_precision: 0.7244 - val_recall: 0.9864\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.51285\n",
            "Epoch 48/100\n",
            " 5400/11911 [============>.................] - ETA: 20s - loss: 0.4699 - accuracy: 0.7630 - precision: 0.7333 - recall: 0.9925"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lM8ytiexrsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8Lny2njrvw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the loss and metrics\n",
        "result = model.evaluate(X_test, y_test)\n",
        "# extract those\n",
        "loss = result[0]\n",
        "accuracy = result[1]\n",
        "precision = result[2]\n",
        "recall = result[3]\n",
        "print(f\"[+] Accuracy: {accuracy*100:.2f}%\") # overall accuracy of model \n",
        "print(f\"[+] Precision:   {precision*100:.2f}%\") # What proportion of positive identifications was actually correct? #https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
        "print(f\"[+] Recall:   {recall*100:.2f}%\") # What proportion of actual positives was identified correctly?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzqzaLhEzHuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_lstm_predict(text): #tokinizes text -> based on that to predict function then depending labels to output thus prediction \n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    # pad the sequence\n",
        "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "    # get the prediction\n",
        "    prediction = model.predict(sequence)   \n",
        "    prob =  model.predict_proba(sequence)[:,1]\n",
        "  \n",
        "    print(prob)\n",
        "    return int2label[np.argmax(prediction)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVhiyw_lzK3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phish Email Test\")\n",
        "print(\"Sextaution Email\")\n",
        "spam_text = \"I’m aware is your password. You may not know me, and you are most likely wondering why you’re getting this mail, right Overview: I installed a malware on the adult vids (sex sites) site, and there’s more, you visited this site to have fun (you know what I mean). Once you were there on the website, my malware took control of your browser. It started operating as a keylogger and remote desktop protocol which gave me access to your webcam. Immediately after that my software collected your complete contacts (you have a good taste lol…)\"\n",
        "print(seq_predict(spam_text))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phish Email Test\")\n",
        "print(\"Paypal Phishing Email\")\n",
        "spam_text1 = \"We’ve noticed some unusual activity on your PayPal account and we’re concerned about the potential unauthorised access, your PayPal account: myemail@gmail.com has be accessed form a new browser or device.\"\n",
        "print(seq_predict(spam_text1))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phishing  Email Test\")\n",
        "print(\"Russian Bride sex scam\")\n",
        "spam_text2 = \"Seems to me that you are the person I am looking for. Tell me about you. Send me your picture as well.I am Lyudmila, I am 38 years old. I wonder how old you are? I am just looking to find a man who might be older when compared to me.I'am living and also came into this world in Russia. What area you live in? I need to get to know a guy with whom i find common pursuits.\"\n",
        "print(rnn_lstm_predict(spam_text2))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"card email\")\n",
        "spam_text3 = \"Are you around? I need to pay a vendor with the blucard.\"\n",
        "print(rnn_lstm_predict(spam_text3))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"login email  email\")\n",
        "spam_text4 = \"Dear Customer,Someone else was trying to use your Berkeley ID to sign into iCloud via a web browser.Date and Time: 28 October 2016,1:38 PMBrowser: FirefoxOperating System: WindowsLocation:ThailandIf the information above looks familiar, youcandisregard this email.If you have not recently and believe someone may be trying to access your account, you should Click Here <http://goo.gl/rk87KW>..\"\n",
        "print(rnn_lstm_predict(spam_text4))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"update  email\")\n",
        "spam_text5 = \"Password will expire in 2 days  Click Here To Validate E-mailThank you,IT-Service Help Desk\"\n",
        "print(rnn_lstm_predict(spam_text5))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"Itunes\")\n",
        "spam_text6 = \"Your Itunes-ID has been disabled.You've place your account under the risk of termination by not keeping the correct informations/Please verify your account as soon as possible.Ready to check ?Click here to get back youraccount\" \n",
        "print(rnn_lstm_predict(spam_text6))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"MADG OFFER\")\n",
        "legit_text = \"Good Afternoon Harry,Further to our conversation I’m pleased to confirm that we would like to make you an offer to join Marshall Aerospace and Defence Group\"\n",
        "print(rnn_lstm_predict(legit_text))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"HSBC Email \")\n",
        "legit_text1 = \"We are making changes to the way we charge you for using your overdraft to comply with new rules that apply to all banks. We explain the changes we’re making in more detail later in this email. It’s important you read these carefully, so you understand what they mean for you\"\n",
        "print(rnn_lstm_predict(legit_text1))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"Exirian  Email \")\n",
        "legit_text2 = \"The health and safety of you and your loved ones is enough to worry about at the moment without adding an increased risk of falling foul to fraud into the mix. Coronavirus-related fraud reports increased by 400% in March, so we wanted to arm you with the tips and advice you need to make sure you’re prepared to spot these underhand scams.\"\n",
        "print(rnn_lstm_predict(legit_text2))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvgHsnoKrl8B",
        "colab_type": "text"
      },
      "source": [
        "bi lstm \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJKSuXBirpyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bi_boi(tokenizer,lstm_units): #loads the function and creates a BI_rnn network \n",
        "  embedding_matrix = get_embedding_vectors(tokenizer)\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len(tokenizer.word_index)+1,\n",
        "                      EMBEDDING_SIZE,\n",
        "                      weights=[embedding_matrix],\n",
        "                      trainable=False,\n",
        "                      input_length =SEQUENCE_LENGTH))\n",
        "  model.add(Bidirectional(LSTM(64))) #creates given network\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(2, activation='sigmoid'))\n",
        "  plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "  model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPi-CnCssBU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_bi_boi = bi_boi(tokenizer,lstm_units=1024)  \n",
        "from keras.utils import plot_model\n",
        "plot_model(model_bi_boi, to_file='model_Bi_lstm.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI2gRrSIsNDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize our ModelCheckpoint and TensorBoard callbacks\n",
        "# model checkpoint for saving best weights\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import History \n",
        "plot_model(model_bi_boi, to_file='model12.png')\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\"phish{val_loss:.2f}\", save_best_only=True,verbose=1)\n",
        "# for better visualization\n",
        "history = History()\n",
        "tensorboard = TensorBoard(f\"phish{time.time()}\")\n",
        "# print our data shapes\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)\n",
        "# traiaining in progress \n",
        "model_bi_boi.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "          callbacks=[tensorboard, model_checkpoint,history],\n",
        "          verbose=1)\n",
        "\n",
        "init_g = tf.global_variables_initializer()\n",
        "init_l = tf.local_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init_g)\n",
        "    sess.run(init_l)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8Thm5CZ8-C1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnQrDYMxyQgQ",
        "colab_type": "text"
      },
      "source": [
        "SAVE THE MODEL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJolvIhjyRjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_bi_boi.save('bi_lstm.h5') #saves the model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H1G2CJoynmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the loss and metrics\n",
        "result = model_bi_boi.evaluate(X_test, y_test)\n",
        "\n",
        "# extract those\n",
        "loss = result[0]\n",
        "accuracy = result[1]\n",
        "precision = result[2]\n",
        "recall = result[3]\n",
        "\n",
        "print(f\"[+] Accuracy: {accuracy*100:.2f}%\") # overall accuracy of model \n",
        "print(f\"[+] Precision:   {precision*100:.2f}%\") # What proportion of positive identifications was actually correct? #https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
        "print(f\"[+] Recall:   {recall*100:.2f}%\") # What proportion of actual positives was identified correctly?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzeDYSIUywmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def bi_lstm_predict(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    # pad the sequence\n",
        "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "    # get the prediction\n",
        "    prediction = model_bi_boi.predict(sequence)[0]\n",
        "    prob =  model.predict_proba(sequence)[:,1]\n",
        "    print (prob)\n",
        "    return int2label[np.argmax(prediction)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUyBrsJyx4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phish Email Test\")\n",
        "print(\"Sextaution Email\")\n",
        "spam_text = \"I’m aware is your password. You may not know me, and you are most likely wondering why you’re getting this mail, right Overview: I installed a malware on the adult vids (sex sites) site, and there’s more, you visited this site to have fun (you know what I mean). Once you were there on the website, my malware took control of your browser. It started operating as a keylogger and remote desktop protocol which gave me access to your webcam. Immediately after that my software collected your complete contacts (you have a good taste lol…)\"\n",
        "print(bi_lstm_predict(spam_text))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phish Email Test\")\n",
        "print(\"Paypal Phishing Email\")\n",
        "spam_text1 = \"We’ve noticed some unusual activity on your PayPal account and we’re concerned about the potential unauthorised access, your PayPal account: myemail@gmail.com has be accessed form a new browser or device.\"\n",
        "print(bi_lstm_predict(spam_text1))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phishing  Email Test\")\n",
        "print(\"Russian Bride sex scam\")\n",
        "spam_text2 = \"Seems to me that you are the person I am looking for. Tell me about you. Send me your picture as well.I am Lyudmila, I am 38 years old. I wonder how old you are? I am just looking to find a man who might be older when compared to me.I'am living and also came into this world in Russia. What area you live in? I need to get to know a guy with whom i find common pursuits.\"\n",
        "print(bi_lstm_predict(spam_text2))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"card email\")\n",
        "spam_text3 = \"Are you around? I need to pay a vendor with the blucard.\"\n",
        "print(bi_lstm_predict(spam_text3))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"login email  email\")\n",
        "spam_text4 = \"Dear User,Someone else was trying to use your Berkeley ID to sign into iCloud via a web browser.Date and Time: 28 October 2016,1:38 PMBrowser: FirefoxOperating System: WindowsLocation:ThailandIf the information above looks familiar, youcandisregard this email.If you have not recently and believe someone may be trying to access your account, you should Click Here <http://goo.gl/rk87KW>..\"\n",
        "print(bi_lstm_predict(spam_text4))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"update  email\")\n",
        "spam_text5 = \"Password will expire in 2 days  Click Here To Validate E-mailThank you,IT-Service Help Desk\"\n",
        "print(bi_lstm_predict(spam_text5))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"login email  email\")\n",
        "spam_text6 = \"Your Itunes-ID has been disabled.You've place your account under the risk of termination by not keeping the correct informations/Please verify your account as soon as possible.Ready to check ?Click here to get back youraccount\" \n",
        "print(bi_lstm_predict(spam_text6))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"MADG OFFER\")\n",
        "legit_text = \"Good Afternoon Harry,Further to our conversation I’m pleased to confirm that we would like to make you an offer to join Marshall Aerospace and Defence Group\"\n",
        "print(bi_lstm_predict(legit_text))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"HSBC Email \")\n",
        "legit_text1 = \"We are making changes to the way we charge you for using your overdraft to comply with new rules that apply to all banks. We explain the changes we’re making in more detail later in this email. It’s important you read these carefully, so you understand what they mean for you\"\n",
        "print(bi_lstm_predict(legit_text1))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"Exirian  Email \")\n",
        "legit_text2 = \"The health and safety of you and your loved ones is enough to worry about at the moment without adding an increased risk of falling foul to fraud into the mix. Coronavirus-related fraud reports increased by 400% in March, so we wanted to arm you with the tips and advice you need to make sure you’re prepared to spot these underhand scams.\"\n",
        "print(bi_lstm_predict(legit_text2))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_1GccJn-8vJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZnFRd_8zPbo",
        "colab_type": "text"
      },
      "source": [
        "Sequencal Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE33WmbRzOcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq_boi(tokenizer): #loads the function and creates a BI_rnn network \n",
        "  embedding_matrix = get_embedding_vectors(tokenizer)\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len(tokenizer.word_index)+1,\n",
        "                      EMBEDDING_SIZE,\n",
        "                      weights=[embedding_matrix],\n",
        "                      trainable=False,\n",
        "                      input_length =SEQUENCE_LENGTH))\n",
        "\n",
        "  from keras.layers import Dense, Activation, Flatten\n",
        "\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "  model.add(MaxPooling1D(pool_size=pool_size))\n",
        "  model.add(LSTM(lstm_output_size))\n",
        "  model.add(Dense(2))\n",
        " \n",
        "  #model.add(Flatten())\n",
        " \n",
        " \n",
        "  \n",
        "  model.add(Dense(2, activation='relu'))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "  model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2uVIysKuQqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "model_seq_boi = seq_boi(tokenizer)  \n",
        "from keras.utils import plot_model\n",
        "plot_model(model_seq_boi, to_file='seq.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YVSjBuNsGaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize our ModelCheckpoint and TensorBoard callbacks\n",
        "# model checkpoint for saving best weights\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_model(model_seq_boi, to_file='model12.png')\n",
        "\n",
        "\n",
        "# for better visualization\n",
        "history = History()\n",
        "\n",
        "# print our data shapes\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)\n",
        "# traiaining in progress \n",
        "history = model_seq_boi.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2AuugHKqRbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the loss and metrics\n",
        "result = model_seq_boi.evaluate(X_test, y_test)\n",
        "\n",
        "# extract those\n",
        "loss = result[0]\n",
        "accuracy = result[1]\n",
        "precision = result[2]\n",
        "recall = result[3]\n",
        "\n",
        "print(f\"[+] Accuracy: {accuracy*100:.2f}%\") # overall accuracy of model \n",
        "print(f\"[+] Precision:   {precision*100:.2f}%\") # What proportion of positive identifications was actually correct? #https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
        "print(f\"[+] Recall:   {recall*100:.2f}%\") # What proportion of actual positives was identified correctly?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz_xfUNeRSLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdgB-IDX7_8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq_predict(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    # pad the sequence\n",
        "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "    # get the prediction\n",
        "    prediction = model_seq_boi.predict(sequence)[0]\n",
        "\n",
        "    return int2label[np.argmax(prediction)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXMPlyjA8X2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phish Email Test\")\n",
        "print(\"Sextaution Email\")\n",
        "spam_text = \"I’m aware is your password. You may not know me, and you are most likely wondering why you’re getting this mail, right Overview: I installed a malware on the adult vids (sex sites) site, and there’s more, you visited this site to have fun (you know what I mean). Once you were there on the website, my malware took control of your browser. It started operating as a keylogger and remote desktop protocol which gave me access to your webcam. Immediately after that my software collected your complete contacts (you have a good taste lol…)\"\n",
        "print(seq_predict(spam_text))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phish Email Test\")\n",
        "print(\"Paypal Phishing Email\")\n",
        "spam_text1 = \"We’ve noticed some unusual activity on your PayPal account and we’re concerned about the potential unauthorised access, your PayPal account: myemail@gmail.com has be accessed form a new browser or device.\"\n",
        "print(seq_predict(spam_text1))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phishing  Email Test\")\n",
        "print(\"Russian Bride sex scam\")\n",
        "spam_text2 = \"Seems to me that you are the person I am looking for. Tell me about you. Send me your picture as well.I am Lyudmila, I am 38 years old. I wonder how old you are? I am just looking to find a man who might be older when compared to me.I'am living and also came into this world in Russia. What area you live in? I need to get to know a guy with whom i find common pursuits.\"\n",
        "print(seq_predict(spam_text2))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"card email\")\n",
        "spam_text3 = \"Are you around? I need to pay a vendor with the blucard.\"\n",
        "print(seq_predict(spam_text3))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"login email  email\")\n",
        "spam_text4 = \"Dear Customer,Someone else was trying to use your Berkeley ID to sign into iCloud via a web browser.Date and Time: 28 October 2016,1:38 PMBrowser: FirefoxOperating System: WindowsLocation:ThailandIf the information above looks familiar, youcandisregard this email.If you have not recently and believe someone may be trying to access your account, you should Click Here <http://goo.gl/rk87KW>..\"\n",
        "print(seq_predict(spam_text4))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"update  email\")\n",
        "spam_text5 = \"Password will expire in 2 days  Click Here To Validate E-mailThank you,IT-Service Help Desk\"\n",
        "print(seq_predict(spam_text5))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"phish\")\n",
        "print(\"Itunes\")\n",
        "spam_text6 = \"Your Itunes-ID has been disabled.You've place your account under the risk of termination by not keeping the correct informations/Please verify your account as soon as possible.Ready to check ?Click here to get back youraccount\" \n",
        "print(seq_predict(spam_text6))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"MADG OFFER\")\n",
        "legit_text = \"Good Afternoon Harry,Further to our conversation I’m pleased to confirm that we would like to make you an offer to join Marshall Aerospace and Defence Group\"\n",
        "print(seq_predict(legit_text))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"HSBC Email \")\n",
        "legit_text1 = \"We are making changes to the way we charge you for using your overdraft to comply with new rules that apply to all banks. We explain the changes we’re making in more detail later in this email. It’s important you read these carefully, so you understand what they mean for you\"\n",
        "print(seq_predict(legit_text1))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"Exirian  Email \")\n",
        "legit_text2 = \"The health and safety of you and your loved ones is enough to worry about at the moment without adding an increased risk of falling foul to fraud into the mix. Coronavirus-related fraud reports increased by 400% in March, so we wanted to arm you with the tips and advice you need to make sure you’re prepared to spot these underhand scams.\"\n",
        "print(seq_predict(legit_text2))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiRH4yfe--IP",
        "colab_type": "text"
      },
      "source": [
        "1d cnn model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LuubL1S_CG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import imdb\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgIGdds5_LIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# set parameters:\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHsUzVh4RpEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r',errors='Ignore')\n",
        "\tlines = file.readlines()\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XT_sNPIRzR6",
        "colab_type": "code",
        "outputId": "46b5af90-5e49-480b-c899-663fea85894f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load embedding from file\n",
        "raw_embedding = get_embedding_vectors(tokenizer)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading GloVe: 16569it [00:01, 14600.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atZSyMvIS8S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = Embedding(BATCH_SIZE,[embedding_dims])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5EZCgYiWT8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_boi(tokenizer): #loads the function and creates a BI_rnn network \n",
        "  embedding_matrix = get_embedding_vectors(tokenizer)\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=maxlen))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "\n",
        "  model.add(Dense(hidden_dims))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "#thinns model to sigmoid large data ---> small Box (squashed) ^_^ \n",
        "  model.add(Dense(2))\n",
        "  model.add(Activation('relu'))\n",
        "  plot_model(model, to_file='1d_cnn.png', show_shapes=True, show_layer_names=True)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Zb48IG6-Fu",
        "colab_type": "code",
        "outputId": "6328b6e9-1c65-4c96-d0fa-03a83b05e070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "cnn_model = cnn_boi(tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading GloVe: 17379it [00:01, 14664.57it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLQXcoqmf5g-",
        "colab_type": "code",
        "outputId": "0f5cc8dd-e980-45cc-eb4c-c1dfe8ff3096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "# initialize our ModelCheckpoint and TensorBoard callbacks\n",
        "# model checkpoint for saving best weights\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import History \n",
        "plot_model(cnn_model, to_file='model12.png')\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\"cnn_model{val_loss:.2f}\", save_best_only=True,verbose=1)\n",
        "# for better visualization\n",
        "history = History()\n",
        "tensorboard = TensorBoard(f\"cnn_model{time.time()}\")\n",
        "# print our data shapes\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)\n",
        "# traiaining in progress \n",
        "hisroty = cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape: (3970, 100)\n",
            "X_test.shape: (3971, 100)\n",
            "y_train.shape: (3970, 2)\n",
            "y_test.shape: (3971, 2)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 3970 samples, validate on 3971 samples\n",
            "Epoch 1/1\n",
            "3970/3970 [==============================] - 7s 2ms/step - loss: 0.8430 - accuracy: 0.5761 - val_loss: 0.6536 - val_accuracy: 0.6487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-bhWqnrmsgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhJckqIuJro5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import *\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RixgrcA3_7Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqoLcQbgVq9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the loss and metrics\n",
        "result = cnn_model.evaluate(X_test, y_test)\n",
        "\n",
        "# extract those\n",
        "loss = result[0]\n",
        "accuracy = result[1]\n",
        "#precision = result[2]\n",
        "#recall = result[3]\n",
        "\n",
        "print(f\"[+] Accuracy: {accuracy*100:.2f}%\") # overall accuracy of model \n",
        "print(f\"[+] Precision:   {precision*100:.2f}%\") # What proportion of positive identifications was actually correct? #https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
        "print(f\"[+] Recall:   {recall*100:.2f}%\") # What proportion of actual positives was identified correctly?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGeLFJgbVNpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def cnn_mod(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    # pad the sequence\n",
        "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "    # get the prediction\n",
        "    prediction = cnn_model.predict(sequence)[0]\n",
        "    prob =  cnn_model.predict_proba(sequence)[0]\n",
        "    print (prob)\n",
        "    return int2label[np.argmax(prediction)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGaw24cNcX0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phish Email Test\")\n",
        "print(\"Sextaution Email\")\n",
        "spam_text = \"I’m aware is your password. You may not know me, and you are most likely wondering why you’re getting this mail, right Overview: I installed a malware on the adult vids (sex sites) site, and there’s more, you visited this site to have fun (you know what I mean). Once you were there on the website, my malware took control of your browser. It started operating as a keylogger and remote desktop protocol which gave me access to your webcam. Immediately after that my software collected your complete contacts (you have a good taste lol…)\"\n",
        "print(cnn_mod(spam_text))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phish Email Test\")\n",
        "print(\"Paypal Phishing Email\")\n",
        "spam_text1 = \"We’ve noticed some unusual activity on your PayPal account and we’re concerned about the potential unauthorised access, your PayPal account: myemail@gmail.com has be accessed form a new browser or device.\"\n",
        "print(cnn_mod(spam_text1))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Phishing  Email Test\")\n",
        "print(\"Russian Bride sex scam\")\n",
        "spam_text2 = \"Seems to me that you are the person I am looking for. Tell me about you. Send me your picture as well.I am Lyudmila, I am 38 years old. I wonder how old you are? I am just looking to find a man who might be older when compared to me.I'am living and also came into this world in Russia. What area you live in? I need to get to know a guy with whom i find common pursuits.\"\n",
        "print(cnn_mod(spam_text2))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"MADG OFFER\")\n",
        "legit_text = \"Good Afternoon Harry,Further to our conversation I’m pleased to confirm that we would like to make you an offer to join Marshall Aerospace and Defence Group\"\n",
        "print(cnn_mod(legit_text))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"HSBC Email \")\n",
        "legit_text1 = \"Hows it going\"\n",
        "print(cnn_mod(legit_text1))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "print(\"=====================\")\n",
        "print (\"Legit Email Test\")\n",
        "print(\"Exirian  Email \")\n",
        "legit_text2 = \"hi hows it going\"\n",
        "print(cnn_mod(legit_text2))\n",
        "print(\"=====================\")\n",
        "print (\"                    \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmabjv9lF5lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model.save('cnnmodel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}